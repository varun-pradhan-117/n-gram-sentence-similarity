{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "14523d1fea9f4e0c7f58fb726c15fbeefb0f5da4b7c94149544d1b99831e5a5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "## Load pre-trained Universal Sentence Encoder(USE) model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(url)"
   ]
  },
  {
   "source": [
    "## Create suite of functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Function to remove special characters from the sentence and return it in a tokenized format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence=re.sub(r\"[^a-zA-Z']\", ' ', sentence)\n",
    "    sentence=re.sub(r\"[^a-zA-Z ]\", '', sentence)\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "source": [
    "### Function to generate n-grams from given sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sentence_tokens,n):\n",
    "    ngram_list=[]\n",
    "    for i in range(1,n+1): \n",
    "        grams=ngrams(sentence_tokens,i)\n",
    "        grams=[' '.join(gram) for gram in grams]\n",
    "        ngram_list.append(grams)\n",
    "    return ngram_list"
   ]
  },
  {
   "source": [
    "### Function to embed the n-grams of given sentence. Zero vector is returned if no n-grams could be formed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_ngrams(ngram_list):\n",
    "    if ngram_list:\n",
    "        return np.array(model(ngram_list))\n",
    "    else:\n",
    "        return np.zeros((1,512))"
   ]
  },
  {
   "source": [
    "### **Function to get similarity score for given n-gram samples**\n",
    "- Since USE returns normalized vectors, the cosine similarity can be calculated through the dot product of two vectors  \n",
    "- The dot product is calculated by matrix multiplication of sentence 1 n-grams with the transpose of sentence 2 n-grams  \n",
    "- Max of all values is taken on the horizontal axis to get the maximum similarity for each n-gram  \n",
    "- Score is calculated by getting the sum of similarities and dividing by number of n-grams in sentence 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sent1_ngrams,sent2_ngrams):\n",
    "    N1=len(sent1_ngrams)\n",
    "    ## Get embedded ngrams\n",
    "    emb_ngrams1=embed_ngrams(sent1_ngrams)\n",
    "    emb_ngrams2=embed_ngrams(sent2_ngrams)\n",
    "\n",
    "    ## Get cosine similarity between n-grams\n",
    "    ## This is done by getting dot product of n-grams of sentence 1 with n-grams of sentence 2\n",
    "    sim_matrix=np.matmul(emb_ngrams1,emb_ngrams2.T)\n",
    "\n",
    "    ## Get maximum similarity score as match\n",
    "    max_sim=np.max(sim_matrix,axis=1)\n",
    "\n",
    "    ## Get score by dividing sum by number of n-grams in sentence 1\n",
    "    score=np.sum(max_sim)/N1\n",
    "    return score"
   ]
  },
  {
   "source": [
    "### Master function to get overall similarity score\n",
    "- Sentences are first cleaned and tokenized\n",
    "- Then sets of n-grams are obtained\n",
    "- The similarity score is calculated for each n-gram\n",
    "- The final sentence similarity is calculated by the weighted sum of the n-gram scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_score(sentence1,sentence2):\n",
    "    # Clean both sentences\n",
    "    cleaned_sent1=clean_sentence(sentence1)\n",
    "    cleaned_sent2=clean_sentence(sentence2)\n",
    "\n",
    "    # Possible solution for different length?\n",
    "    if len(cleaned_sent2)>len(cleaned_sent1):\n",
    "       cleaned_sent1,cleaned_sent2=cleaned_sent2,cleaned_sent1\n",
    "    n=3\n",
    "    # Get unigrams, bigrams and trigrams for sentence 1 and sentence 2\n",
    "    sent1_ngrams=get_ngrams(cleaned_sent1,n)\n",
    "    sent2_ngrams=get_ngrams(cleaned_sent2,n)\n",
    "    final_score=0\n",
    "    div=6\n",
    "    # n-gram score\n",
    "    for i in range(0,n):\n",
    "        scoren=get_score(sent1_ngrams[i],sent2_ngrams[i])\n",
    "        print(f'{i+1}-gram score: {scoren}')\n",
    "        # Multiply scoren with weight and add to final score\n",
    "        final_score+=(i+1)*scoren\n",
    "    # Divide final score by the common divisor\n",
    "    return final_score/div"
   ]
  },
  {
   "source": [
    "### Function to get most similar sentence from a list of sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_score(input_sentence,sentence_list):\n",
    "    score_list=[]\n",
    "    # iterate over list of sentences to get a list of scores\n",
    "    for sentence in sentence_list:\n",
    "        print(f'Comparing with : {sentence}')\n",
    "        score=get_sim_score(input_sentence,sentence)\n",
    "        print(\"Final score = \",score)\n",
    "        print('-----------------------')\n",
    "        score_list.append(score)\n",
    "    # Select highest similarity score\n",
    "    best_score=max(score_list)\n",
    "    # Get sentence with highest similarity\n",
    "    best_sentence=sentence_list[score_list.index(best_score)]\n",
    "    return best_sentence,best_score"
   ]
  },
  {
   "source": [
    "### Calling function on a few samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1-gram score: 0.8474415370396206\n2-gram score: 0.7590710322062174\n3-gram score: 0.6750203132629394\nSentence similarity = 0.7317740902068123\n"
     ]
    }
   ],
   "source": [
    "sentence_sim=get_sim_score(\"I hope you received my mail\",\"I hope you have read my mail\")\n",
    "print(\"Sentence similarity =\",sentence_sim)"
   ]
  },
  {
   "source": [
    "### Testing with similar words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Comparing with : Cancel my order\n",
      "1-gram score: 0.6111734708150228\n",
      "2-gram score: 0.5327193140983582\n",
      "3-gram score: 0.3417583107948303\n",
      "Final score =  0.45031450523270505\n",
      "-----------------------\n",
      "Comparing with : Show recent items\n",
      "1-gram score: 0.466095765431722\n",
      "2-gram score: 0.2164982706308365\n",
      "3-gram score: 0.1797012984752655\n",
      "Final score =  0.23969936701986524\n",
      "-----------------------\n",
      "Comparing with : Show my orders\n",
      "1-gram score: 0.6386563777923584\n",
      "2-gram score: 0.6075990796089172\n",
      "3-gram score: 0.4083702862262726\n",
      "Final score =  0.5131608992815018\n",
      "-----------------------\n",
      "Comparing with : Track my order\n",
      "1-gram score: 0.6304430961608887\n",
      "2-gram score: 0.6145361661911011\n",
      "3-gram score: 0.5404067039489746\n",
      "Final score =  0.5801225900650024\n",
      "-----------------------\n",
      "Comparing with : Confirm my order\n",
      "1-gram score: 0.6111734708150228\n",
      "2-gram score: 0.5035542249679565\n",
      "3-gram score: 0.2844642400741577\n",
      "Final score =  0.4119457734955682\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "sentences=[\"Cancel my order\",\n",
    "           \"Show recent items\",\n",
    "           \"Show my orders\",\n",
    "           \"Track my order\",\n",
    "           \"Confirm my order\"]\n",
    "sentence,score=get_best_score(\"Find my package\",sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input is most similar to 'Track my order' with a score of 0.5801225900650024\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input is most similar to '{sentence}' with a score of {score}\")"
   ]
  },
  {
   "source": [
    "### Testing with very different sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Comparing with : Cancel my order\n",
      "1-gram score: 0.5626726150512695\n",
      "2-gram score: 0.4326210916042328\n",
      "3-gram score: 0.30491383870442706\n",
      "Final score =  0.3904427190621694\n",
      "-----------------------\n",
      "Comparing with : Show recent items\n",
      "1-gram score: 0.36089043617248534\n",
      "2-gram score: 0.14835841953754425\n",
      "3-gram score: 0.056354264418284096\n",
      "Final score =  0.13777834475040437\n",
      "-----------------------\n",
      "Comparing with : Show my orders\n",
      "1-gram score: 0.5521992683410645\n",
      "2-gram score: 0.39841729402542114\n",
      "3-gram score: 0.21370969216028848\n",
      "Final score =  0.331693822145462\n",
      "-----------------------\n",
      "Comparing with : Track my order\n",
      "1-gram score: 0.5742343902587891\n",
      "2-gram score: 0.3852463960647583\n",
      "3-gram score: 0.24673974514007568\n",
      "Final score =  0.34749106963475546\n",
      "-----------------------\n",
      "Comparing with : Confirm my order\n",
      "1-gram score: 0.5626726150512695\n",
      "2-gram score: 0.3768530488014221\n",
      "3-gram score: 0.1868083874384562\n",
      "Final score =  0.31280064582824707\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "sentence2,score2=get_best_score(\"I don't want my package\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input is most similar to 'Cancel my order' with a score of 0.3904427190621694\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input is most similar to '{sentence2}' with a score of {score2}\")"
   ]
  }
 ]
}