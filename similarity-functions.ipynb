{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf-gpu': conda)"
  },
  "interpreter": {
   "hash": "14523d1fea9f4e0c7f58fb726c15fbeefb0f5da4b7c94149544d1b99831e5a5d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Import dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from itertools import repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "source": [
    "## Load pre-trained Universal Sentence Encoder(USE) model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model = hub.load(url)"
   ]
  },
  {
   "source": [
    "## Create suite of functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Function to remove special characters from the sentence and return it in a tokenized format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence=re.sub(r\"[^a-zA-Z']\", ' ', sentence)\n",
    "    sentence=re.sub(r\"[^a-zA-Z ]\", '', sentence)\n",
    "    return word_tokenize(sentence)"
   ]
  },
  {
   "source": [
    "### Function to generate unigrams, bigrams and trigrams from given sentence"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sentence_tokens):\n",
    "    unigrams=ngrams(sentence_tokens,1)\n",
    "    unigrams=[' '.join(gram) for gram in unigrams]\n",
    "    bigrams=ngrams(sentence_tokens,2)\n",
    "    bigrams=[' '.join(gram) for gram in bigrams]\n",
    "    trigrams=ngrams(sentence_tokens,3)\n",
    "    trigrams=[' '.join(gram) for gram in trigrams]\n",
    "    return unigrams,bigrams,trigrams\n",
    "    "
   ]
  },
  {
   "source": [
    "### Function to embed the n-grams of given sentence. Zero vector is returned if no n-grams could be formed"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_ngrams(ngram_list):\n",
    "    if ngram_list:\n",
    "        return np.array(model(ngram_list))\n",
    "    else:\n",
    "        return np.zeros((1,512))"
   ]
  },
  {
   "source": [
    "### **Function to get similarity score for given n-gram samples**\n",
    "- Since USE returns normalized vectors, the cosine similarity can be calculated through the dot product of two vectors  \n",
    "- The dot product is calculated by matrix multiplication of sentence 1 n-grams with the transpose of sentence 2 n-grams  \n",
    "- Max of all values is taken on the horizontal axis to get the maximum similarity for each n-gram  \n",
    "- Score is calculated by getting the sum of similarities and dividing by number of n-grams in sentence 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(sent1_ngrams,sent2_ngrams):\n",
    "    N1=len(sent1_ngrams)\n",
    "    ## Get embedded ngrams\n",
    "    emb_ngrams1=embed_ngrams(sent1_ngrams)\n",
    "    emb_ngrams2=embed_ngrams(sent2_ngrams)\n",
    "\n",
    "    ## Get cosine similarity between n-grams\n",
    "    ## This is done by getting dot product of n-grams of sentence 1 with n-grams of sentence 2\n",
    "    sim_matrix=np.matmul(emb_ngrams1,emb_ngrams2.T)\n",
    "\n",
    "    ## Get maximum similarity score as match\n",
    "    max_sim=np.max(sim_matrix,axis=1)\n",
    "\n",
    "    ## Get score by dividing sum by number of n-grams in sentence 1\n",
    "    score=np.sum(max_sim)/N1\n",
    "    return score"
   ]
  },
  {
   "source": [
    "### Master function to get overall similarity score\n",
    "- Sentences are first cleaned and tokenized\n",
    "- Then unigrams, bigrams and trigrams are obtained\n",
    "- The similarity score is calculated for each n-gram\n",
    "- The final sentence similarity is calculated by the weighted sum of the n-gram scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_score(sentence1,sentence2):\n",
    "    # Clean both sentences\n",
    "    cleaned_sent1=clean_sentence(sentence1)\n",
    "    cleaned_sent2=clean_sentence(sentence2)\n",
    "    # Possible solution for different length?\n",
    "    if len(cleaned_sent2)>len(cleaned_sent1):\n",
    "       cleaned_sent1,cleaned_sent2=cleaned_sent2,cleaned_sent1\n",
    "\n",
    "    # Get unigrams, bigrams and trigrams for sentence 1 and sentence 2\n",
    "    sent1_unigrams,sent1_bigrams,sent1_trigrams=get_ngrams(cleaned_sent1)\n",
    "    sent2_unigrams,sent2_bigrams,sent2_trigrams=get_ngrams(cleaned_sent2)\n",
    "\n",
    "    # Unigram Score\n",
    "    score1=get_score(sent1_unigrams,sent2_unigrams)\n",
    "    print(\"Unigram score:\",score1)\n",
    "    print(\"_________________________________\")\n",
    "    # Bigram score\n",
    "    score2=get_score(sent1_bigrams,sent2_bigrams)\n",
    "    print(\"Bigram score:\",score2)\n",
    "    print(\"_________________________________\")\n",
    "    # Trigram Score\n",
    "    score3=get_score(sent1_trigrams,sent2_trigrams)\n",
    "    print(\"Trigram score:\",score3)\n",
    "    print(\"_________________________________\")\n",
    "    sentence_similarity=  1*score1/6 + 2*score2/6 + 3*score3/6\n",
    "    return sentence_similarity"
   ]
  },
  {
   "source": [
    "### Calling function on a few samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unigram score: 0.8474415370396206\n_________________________________\nBigram score: 0.7590710322062174\n_________________________________\nTrigram score: 0.6750203132629394\n_________________________________\nSentence similarity= 0.7317740902068124\n"
     ]
    }
   ],
   "source": [
    "sentence_sim=get_sim_score(\"I hope you received my mail\",\"I hope you have read my mail\")\n",
    "print(\"Sentence similarity=\",sentence_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unigram score: 0.5047336101531983\n_________________________________\nBigram score: 0.40157806873321533\n_________________________________\nTrigram score: 0.3316713372866313\n_________________________________\nSentence similarity= 0.38381729324658714\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence similarity=\",get_sim_score(\"Cancel my order please\",\"Show my last bought items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unigram score: 0.7891566753387451\n_________________________________\nBigram score: 0.7568678174700055\n_________________________________\nTrigram score: 0.7355847358703613\n_________________________________\nSentence similarity= 0.7516077529816401\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence similarity=\",get_sim_score(\"I don't want my order\",\"I don't want my order to be delayed\"))"
   ]
  }
 ]
}